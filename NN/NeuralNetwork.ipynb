{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Neural Network from Scratch.\n",
    "Neural Networks are really powerful algorithms used for classification. <br>\n",
    "Dataset = Iris_Dataset <br>\n",
    "Link = http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html <br>\n",
    "\n",
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets     #for dataset\n",
    "import numpy as np               #for maths\n",
    "import matplotlib.pyplot as plt  #for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape   = (150, 4)\n",
      "Target Shape = (150, 1)\n",
      "Classes : [0 1 2]\n",
      "Sample data : [ 5.9  3.2  4.8  1.8] , Target = [1]\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()      #load the dataset\n",
    "data = iris.data                 #get features  \n",
    "target = iris.target             #get labels\n",
    "\n",
    "shape = data.shape               #shape of data\n",
    "\n",
    "#convert into numpy array\n",
    "data = np.array(data).reshape(shape[0],shape[1])\n",
    "target = np.array(target).reshape(shape[0],1)\n",
    "\n",
    "#print shape\n",
    "print(\"Data Shape   = {}\".format(data.shape))\n",
    "print(\"Target Shape = {}\".format(target.shape))\n",
    "print('Classes : {}'.format(np.unique(target)))\n",
    "print('Sample data : {} , Target = {}'.format(data[70],target[70]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters and Hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hidden layer Neural Network.\n",
    " ![](https://github.com/navjindervirdee/neural-networks/blob/master/Neural%20Network/network.JPG?raw=true)",
    "\n",
    "Input Units  = 4 <br>\n",
    "Hidden Units = 8 <br>\n",
    "Output Units = 3 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "\n",
    "#num of target labels\n",
    "num_classes = len(np.unique(target))\n",
    "\n",
    "#define layer_neurons\n",
    "input_units  = 4   #neurons in input layer\n",
    "hidden_units = 8   #neurons in hidden layer\n",
    "output_units = 3   #neurons in output layer\n",
    "\n",
    "#define hyper-parameters\n",
    "learning_rate = 0.03\n",
    "\n",
    "#regularization parameter\n",
    "beta = 0.00001\n",
    "\n",
    "#num of iterations\n",
    "iters = 4001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimesions of Parameters\n",
    "Shape of layer1_weights (Wxh) = (4,8)  <br>\n",
    "Shape of layer1_biasess (Bh) = (8,1)  <br>\n",
    "Shape of layer2_weights (Why) = (8,3)  <br>\n",
    "Shape of layer2_biasess (By) = (3,1)  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PARAMETERS\n",
    "\n",
    "#initialize parameters i.e weights\n",
    "def initialize_parameters():\n",
    "    #initial values should have zero mean and 0.1 standard deviation\n",
    "    mean = 0        #mean of parameters \n",
    "    std = 0.03      #standard deviation\n",
    "    \n",
    "    layer1_weights = np.random.normal(mean,std,(input_units,hidden_units))          \n",
    "    layer1_biases = np.ones((hidden_units,1))                                       \n",
    "    layer2_weights = np.random.normal(mean,std,(hidden_units,output_units))\n",
    "    layer2_biases = np.ones((output_units,1))\n",
    "    \n",
    "    parameters = dict()\n",
    "    parameters['layer1_weights'] = layer1_weights\n",
    "    parameters['layer1_biases'] = layer1_biases\n",
    "    parameters['layer2_weights'] = layer2_weights\n",
    "    parameters['layer2_biases'] = layer2_biases\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "**Sigmoid**\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#activation function\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp((-1)*X))\n",
    "\n",
    "#softmax function for output\n",
    "def softmax(X):\n",
    "    exp_X = np.exp(X)\n",
    "    exp_X_sum = np.sum(exp_X,axis=1).reshape(-1,1)\n",
    "    exp_X = (exp_X/exp_X_sum)\n",
    "    return exp_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Utility Functions\n",
    "#### 1. Forward Propagation\n",
    "---- Logits = matmul(X,Wxh) + Bh       <br>\n",
    "---- A = sigmoid(logits)       <br>\n",
    "---- logits = matmul(A,Why) + By       <br>\n",
    "---- output = softmax(logits)  <br>\n",
    "\n",
    "Store output and A in cache to use it in backward propagation <br>\n",
    "\n",
    "#### 2. Backward Propagation\n",
    "---- Error_output = output - train_labels <br>\n",
    "---- Error_activation = (matmul(error_output,Why.T))(A)(1-A) <br>\n",
    "---- dWhy = (matmul(A.T,error_output))/m <br>\n",
    "---- dWxh = (matmul(train_dataset.T,error_activation))/m <br>\n",
    "\n",
    "m = len(train_dataset) <br>\n",
    "Store derivatives in derivatives dict\n",
    "\n",
    "#### 3. Update Parameters\n",
    "---- Wxh = Wxh - learning_rate(dWxh + beta*Wxh) <br>\n",
    "---- Why = Why - learning_rate(dWhy + beta*Why) <br>\n",
    "\n",
    "#### 4. Calculate Loss and Accuracy\n",
    "---- Loss = (-1(Y log(prediction)) + (1-Y) (log(1-predictions)))  + beta * (sum(Wxh^2) + sum(Why^2)))/m  <br>\n",
    "---- Accuracy = sum(Y==predictions)/m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propagation\n",
    "def forward_propagation(train_dataset,parameters):\n",
    "    cache = dict()            #to store the intermediate values for backward propagation\n",
    "    m = len(train_dataset)    #number of training examples\n",
    "    \n",
    "    #get the parameters\n",
    "    layer1_weights = parameters['layer1_weights']\n",
    "    layer1_biases = parameters['layer1_biases']\n",
    "    layer2_weights = parameters['layer2_weights']\n",
    "    layer2_biases = parameters['layer2_biases']\n",
    "    \n",
    "    #forward prop\n",
    "    logits = np.matmul(train_dataset,layer1_weights) + layer1_biases.T\n",
    "    activation1 = np.array(sigmoid(logits)).reshape(m,hidden_units)\n",
    "    activation2 = np.array(np.matmul(activation1,layer2_weights) + layer2_biases.T).reshape(m,output_units)\n",
    "    output = np.array(softmax(activation2)).reshape(m,num_classes)\n",
    "    \n",
    "    #fill in the cache\n",
    "    cache['output'] = output\n",
    "    cache['activation1'] = activation1\n",
    "    \n",
    "    return cache,output\n",
    "\n",
    "#backward propagation\n",
    "def backward_propagation(train_dataset,train_labels,parameters,cache):\n",
    "    derivatives = dict()         #to store the derivatives\n",
    "    \n",
    "    #get stuff from cache\n",
    "    output = cache['output']\n",
    "    activation1 = cache['activation1']\n",
    "    \n",
    "    #get parameters\n",
    "    layer1_weights = parameters['layer1_weights']\n",
    "    layer2_weights = parameters['layer2_weights']\n",
    "    \n",
    "    #calculate errors\n",
    "    error_output = output - train_labels\n",
    "    error_activation1 = np.matmul(error_output,layer2_weights.T)\n",
    "    error_activation1 = np.multiply(error_activation1,activation1)\n",
    "    error_activation1 = np.multiply(error_activation1,1-activation1)\n",
    "    \n",
    "    \n",
    "    #calculate partial derivatives\n",
    "    partial_derivatives2 = np.matmul(activation1.T,error_output)/len(train_dataset)\n",
    "    partial_derivatives1 = np.matmul(train_dataset.T,error_activation1)/len(train_dataset)\n",
    "    \n",
    "    #store the derivatives\n",
    "    derivatives['partial_derivatives1'] = partial_derivatives1\n",
    "    derivatives['partial_derivatives2'] = partial_derivatives2\n",
    "    \n",
    "    return derivatives\n",
    "\n",
    "\n",
    "#update the parameters\n",
    "def update_parameters(derivatives,parameters):\n",
    "    #get the parameters\n",
    "    layer1_weights = parameters['layer1_weights']\n",
    "    layer2_weights = parameters['layer2_weights']\n",
    "    \n",
    "    #get the derivatives\n",
    "    partial_derivatives1 = derivatives['partial_derivatives1']\n",
    "    partial_derivatives2 = derivatives['partial_derivatives2']\n",
    "    \n",
    "    #update the derivatives\n",
    "    layer1_weights -= (learning_rate*(partial_derivatives1 + beta*layer1_weights))\n",
    "    layer2_weights -= (learning_rate*(partial_derivatives2 + beta*layer2_weights))\n",
    "    \n",
    "    #update the dict\n",
    "    parameters['layer1_weights'] = layer1_weights\n",
    "    parameters['layer2_weights'] = layer2_weights\n",
    "    \n",
    "    return parameters\n",
    "    \n",
    "#calculate the loss and accuracy\n",
    "def cal_loss_accuray(train_labels,predictions,parameters):\n",
    "    #get the parameters\n",
    "    layer1_weights = parameters['layer1_weights']\n",
    "    layer2_weights = parameters['layer2_weights']\n",
    "    \n",
    "    #cal loss and accuracy\n",
    "    loss = -1*np.sum(np.multiply(np.log(predictions),train_labels) + np.multiply(np.log(1-predictions),(1-train_labels)))/len(train_labels) + np.sum(layer1_weights**2)*beta/len(train_labels) + np.sum(layer2_weights**2)*beta/len(train_labels)\n",
    "    accuracy = np.sum(np.argmax(train_labels,axis=1)==np.argmax(predictions,axis=1))\n",
    "    accuracy /= len(train_dataset)\n",
    "    \n",
    "    return loss,accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function\n",
    "\n",
    "1. Initialize Parameters\n",
    "2. Forward Propagation\n",
    "3. Backward Propagation\n",
    "4. Calculate Loss and Accuracy\n",
    "5. Update the parameters\n",
    "\n",
    "Repeat the steps 2-5 for the given number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implementation of 3 layer Neural Network\n",
    "\n",
    "#training function\n",
    "def train(train_dataset,train_labels,iters=2):\n",
    "    #To store loss after every iteration.\n",
    "    J = []\n",
    "  \n",
    "    #WEIGHTS\n",
    "    global layer1_weights\n",
    "    global layer1_biases\n",
    "    global layer2_weights\n",
    "    global layer2_biases\n",
    "  \n",
    "    #initialize the parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    layer1_weights = parameters['layer1_weights']\n",
    "    layer1_biases = parameters['layer1_biases']\n",
    "    layer2_weights = parameters['layer2_weights']\n",
    "    layer2_biases = parameters['layer2_biases']\n",
    "    \n",
    "    #to store final predictons after training\n",
    "    final_output = []\n",
    "    \n",
    "    for j in range(iters):\n",
    "        #forward propagation\n",
    "        cache,output = forward_propagation(train_dataset,parameters)\n",
    "        \n",
    "        #backward propagation\n",
    "        derivatives = backward_propagation(train_dataset,train_labels,parameters,cache)\n",
    "        \n",
    "        #calculate the loss and accuracy\n",
    "        loss,accuracy = cal_loss_accuray(train_labels,output,parameters)\n",
    "        \n",
    "        #update the parameters\n",
    "        parameters = update_parameters(derivatives,parameters)\n",
    "        \n",
    "        #append loss\n",
    "        J.append(loss)\n",
    "        \n",
    "        #update final output\n",
    "        final_output = output\n",
    "        \n",
    "        #print accuracy and loss\n",
    "        if(j%500==0):\n",
    "            print(\"Step %d\"%j)\n",
    "            print(\"Loss %f\"%loss)\n",
    "            print(\"Accuracy %f%%\"%(accuracy*100))\n",
    "    \n",
    "    return J,final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the dataset\n",
    "z = list(zip(data,target))\n",
    "np.random.shuffle(z)\n",
    "data,target = zip(*z)\n",
    "\n",
    "#make train_dataset and train_labels\n",
    "train_dataset = np.array(data).reshape(-1,4)\n",
    "train_labels = np.zeros([train_dataset.shape[0],num_classes])\n",
    "\n",
    "#one-hot encoding\n",
    "for i,label in enumerate(target):\n",
    "    train_labels[i,label] = 1\n",
    "\n",
    "#normalizations\n",
    "for i in range(input_units):\n",
    "    mean = train_dataset[:,i].mean()\n",
    "    std = train_dataset[:,i].std()\n",
    "    train_dataset[:,i] = (train_dataset[:,i]-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Loss 1.911767\n",
      "Accuracy 33.333333%\n",
      "Step 500\n",
      "Loss 1.772284\n",
      "Accuracy 76.666667%\n",
      "Step 1000\n",
      "Loss 0.934929\n",
      "Accuracy 93.333333%\n",
      "Step 1500\n",
      "Loss 0.648656\n",
      "Accuracy 93.333333%\n",
      "Step 2000\n",
      "Loss 0.504929\n",
      "Accuracy 96.000000%\n",
      "Step 2500\n",
      "Loss 0.413799\n",
      "Accuracy 96.666667%\n",
      "Step 3000\n",
      "Loss 0.348909\n",
      "Accuracy 96.666667%\n",
      "Step 3500\n",
      "Loss 0.301780\n",
      "Accuracy 96.000000%\n",
      "Step 4000\n",
      "Loss 0.267607\n",
      "Accuracy 97.333333%\n"
     ]
    }
   ],
   "source": [
    "#train data\n",
    "J,final_output = train(train_dataset,train_labels,iters=4001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reached an Accuracy of 97%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss vs iteration graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTP4/QsdovYcuFO4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x265cffff860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot loss graph\n",
    "plt.plot(list(range(1,len(J))),J[1:])\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Iterations VS Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
